# Default configuration for PPO training
defaults:
  - _self_
  - env: track1
  - reward: lift
  - control: single_arm  # Use dual_arm for sort task
  - obs: single_arm      # Use dual_arm for sort task

# Experiment settings
exp_name: null
seed: 1

# Logging
wandb:
  enabled: true
  project: "Track1-PPO"
  entity: null

# Optimization flags
compile: true
cudagraphs: true
anneal_lr: true

# Training hyperparameters
training:
  total_timesteps: 1_000_000_000
  num_envs: 2048
  num_eval_envs: 8
  num_steps: 50
  num_eval_steps: null
  eval_step_multiplier: 1.0
  eval_freq: 25

# PPO hyperparameters
ppo:
  learning_rate: 3e-4
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 32
  update_epochs: 4
  clip_coef: 0.2
  clip_vloss: false      # Whether to clip value loss (as per original PPO paper)
  norm_adv: true        # Whether to normalize advantages per minibatch
  ent_coef: 0.005
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: 0.2
  reward_scale: 1.0
  handle_timeout_termination: true  # If true, bootstrap on truncation (recommended)
  logstd_init: -1.0      # Initial log standard deviation (e.g. 0.0 = std 1.0, -0.5 = std 0.6)
  logstd_min: -5.0       # Minimum log standard deviation clamp
  logstd_max: 0.0        # Maximum log standard deviation clamp

# Model saving
save_model: true
capture_video: true
checkpoint: null

# Debugging/Monitoring
log_obs_stats: true  # Log mean/std of observation inputs (useful for monitoring normalization)
normalize_obs: true  # Whether to apply running normalization to observations
init_obs_stats_from_config: true  # Initialize obs stats from config to stabilize early training
obs_stats_tau: 0.001   # EMA decay rate for obs statistics (higher = faster adaptation)
async_eval: true      # Run evaluation in background thread (non-blocking)

# Reward Normalization (like gym.wrappers.NormalizeReward, but GPU-compatible)
normalize_reward: true  # Whether to apply running reward normalization (disable by default)
reward_clip: 10.0       # Clip normalized rewards to [-clip, clip]
