# Default configuration for PPO training
defaults:
  - _self_
  - env: track1
  - reward: lift
  - control: single_arm  # Use dual_arm for sort task
  - obs: single_arm      # Use dual_arm for sort task

# Experiment settings
exp_name: null
seed: 1

# Logging
wandb:
  enabled: true
  project: "Track1-PPO"
  entity: null

# Optimization flags
compile: true
cudagraphs: true
anneal_lr: true

# Training hyperparameters
training:
  total_timesteps: 1_000_000_000
  num_envs: 2048
  num_eval_envs: 8
  num_steps: 50
  num_eval_steps: null
  eval_step_multiplier: 1.0
  eval_freq: 25

# PPO hyperparameters
ppo:
  learning_rate: 1e-4
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 32
  update_epochs: 4
  clip_coef: 0.2
  clip_vloss: false      # Whether to clip value loss (as per original PPO paper)
  norm_adv: true        # Whether to normalize advantages per minibatch
  ent_coef: 0.005
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: 0.015      # Reduced from 0.2 to prevent policy collapse (Standard PPO: 0.01-0.02)
  reward_scale: 1.0
  handle_timeout_termination: true  # If true, bootstrap on truncation (recommended)
  logstd_init: -1.0      # Initial log standard deviation (e.g. 0.0 = std 1.0, -0.5 = std 0.6)
  logstd_min: -5.0       # Minimum log standard deviation clamp
  logstd_max: 0.0        # Maximum log standard deviation clamp

# Model saving
save_model: true
capture_video: true
checkpoint: null

# Debugging/Monitoring
log_obs_stats: true  # Log mean/std of observation inputs (useful for monitoring normalization)
async_eval: true     # Run evaluation in background thread (non-blocking)

# Observation Normalization (like gym.wrappers.NormalizeObservation, but GPU-compatible)
normalize_obs: true   # Whether to apply running normalization to observations (Welford's online algorithm)
obs_clip: 10.0        # Clip normalized observations to [-clip, clip]
# Note: Manual mean/std initialization from obs config is deprecated - RunningMeanStd adapts automatically
init_obs_stats_from_config: false  # [DEPRECATED] No longer used with Welford's algorithm
obs_stats_tau: 0.001  # EMA decay rate for additional obs statistics tracking (for monitoring only)

# Reward Normalization (like gym.wrappers.NormalizeReward, but GPU-compatible)
normalize_reward: true  # Whether to apply running reward normalization (disable by default)
reward_clip: 10.0       # Clip normalized rewards to [-clip, clip]
