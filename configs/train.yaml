# Default configuration for PPO training
defaults:
  - _self_
  - env: track1
  - reward: lift
  - control: single_arm  # Use dual_arm for sort task

# Experiment settings
exp_name: null
seed: 1

# Logging
wandb:
  enabled: true
  project: "Track1-PPO"
  entity: null

# Optimization flags
compile: true
cudagraphs: false
anneal_lr: true

# Training hyperparameters
training:
  total_timesteps: 10_000_000
  num_envs: 128
  num_eval_envs: 8
  num_steps: 50
  num_eval_steps: 100
  eval_freq: 25

# PPO hyperparameters
ppo:
  learning_rate: 3e-4
  gamma: 0.8
  gae_lambda: 0.9
  num_minibatches: 32
  update_epochs: 4
  clip_coef: 0.2
  clip_vloss: true      # Whether to clip value loss (as per original PPO paper)
  norm_adv: true        # Whether to normalize advantages per minibatch
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: 0.2
  reward_scale: 1.0
  handle_timeout_termination: true  # If true, bootstrap on truncation (recommended)

# Model saving
save_model: true
capture_video: true
checkpoint: null
